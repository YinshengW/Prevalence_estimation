{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Mental Health Prevalence Analysis and Mapping for Washington State\n",
    "- Author: [Your Name]\n",
    "- Date: [Current Date]\n",
    "\n",
    "This script analyzes and maps the prevalence of mental health conditions\n",
    "among adolescents (14-17 years) in Washington State using ML predictions\n",
    "constrained by demographic data from the American Community Survey (ACS).\n",
    "\n",
    "Requirements:\n",
    "- pandas, numpy, matplotlib, geopandas\n",
    "- scikit-learn, xgboost, lightgbm, imblearn\n",
    "- gurobipy for optimization\n",
    "- census API access\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import itertools\n",
    "import pickle\n",
    "import os\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Census API\n",
    "from census import Census\n",
    "import us\n",
    "\n",
    "# Optimization library\n",
    "import gurobipy as gp\n",
    "from gurobipy import Model, GRB, quicksum\n",
    "\n",
    "# Machine learning models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "import shap\n",
    "\n",
    "# Configure API key for Census data\n",
    "CENSUS_API_KEY = \"ed75ad333b249bcbda60ef3fbae2fc722c7a7b5e\"\n",
    "c = Census(CENSUS_API_KEY)\n",
    "\n",
    "#################################################\n",
    "# CONFIGURATION AND HELPER FUNCTIONS\n",
    "#################################################\n",
    "\n",
    "# Define variable mappings for NSCH dataset\n",
    "name2col = {\n",
    "    # Mental health outcomes\n",
    "    'Anxiety': 'K2Q33A',          # 1 yes, 2 no\n",
    "    'Depression': 'K2Q32A',       # 1 yes, 2 no\n",
    "    'ADHD': 'K2Q31A',             # 1 yes, 2 no\n",
    "    'Behavior Problems': 'K2Q34A', # 1 yes, 2 no\n",
    "    \n",
    "    # Demographics\n",
    "    'Age': 'SC_AGE_YEARS',         # 1~17\n",
    "    'Grouped age': 'SC_AGE_GROUP', # 1~4\n",
    "    'Race': 'race7_21',            # 1~7\n",
    "    'Sex': 'SC_SEX',               # 1 male, 2 female\n",
    "    'Highest education': 'HIGRADE_TVIS', # 1~4\n",
    "    'Language': 'HHLanguage_21',   # 1 primarily English, 2 others\n",
    "    'Family size': 'FamCount_21',  # 1~3\n",
    "    'Insurance': 'InsGap_21',      # 1 yes, 2 no\n",
    "    'Employment': 'EmploymentSt_21', # 1 full-time, 2 part-time, 3 unemployed\n",
    "    'Family income': 'ACEincome_21', # 1~4\n",
    "    'Family structure': 'famstruct5_21', # 1~5\n",
    "    'Parents mental health': 'A1_MENTHEALTH',\n",
    "    'Parents born in US': 'A1_BORN',\n",
    "}\n",
    "\n",
    "# Reverse mapping\n",
    "col2name = {v: k for k, v in name2col.items()}\n",
    "\n",
    "# Variables used to train the model\n",
    "NSCH_ACS_variables = ['Grouped age', 'Sex', 'Race', 'Highest education', \n",
    "                      'Language', 'Family size', 'Insurance', 'Employment', \n",
    "                      'Family income', 'Family structure']\n",
    "\n",
    "# Feature variables mapping used for analysis\n",
    "name2col_feature = {k: v for k, v in name2col.items() if k in NSCH_ACS_variables}\n",
    "\n",
    "# Dictionary for demographic value meanings\n",
    "var2value2name = {\n",
    "    'Race': {\n",
    "        1: 'Hispanic',\n",
    "        2: 'White',\n",
    "        3: 'Black',\n",
    "        4: 'Asian',\n",
    "        5: 'Others'\n",
    "    },\n",
    "    'Sex': {\n",
    "        1: 'male',\n",
    "        2: 'female'\n",
    "    },\n",
    "    'Highest education': {\n",
    "        1: 'Less than high school',\n",
    "        2: 'High school or GED',\n",
    "        3: 'Some college or technical school',\n",
    "        4: 'College degree or higher',\n",
    "    },\n",
    "    'Language': {\n",
    "        1: 'primarily English',\n",
    "        2: 'others'\n",
    "    },\n",
    "    'Insurance': {\n",
    "        1: 'Consistantly insured over the past year',\n",
    "        2: 'Consistantly uninsured or had periods without coverage'\n",
    "    },\n",
    "    'Employment': {\n",
    "        1: 'At least one caregiver employed full-time or part-time',\n",
    "        2: 'Caregiver(s) unemployed or working without pay'\n",
    "    },\n",
    "    'Family income': {\n",
    "        1: 'Above the poverty line',\n",
    "        2: 'Below the poverty line'\n",
    "    },\n",
    "    'Family structure': {\n",
    "        1: 'Two parents, currently married',\n",
    "        2: 'Two parents, not currently married',\n",
    "        3: 'Single parent (mother or father)',\n",
    "        4: 'Other family type'\n",
    "    },\n",
    "}\n",
    "\n",
    "# List of features used in analysis\n",
    "used_vars = list(var2value2name.keys())\n",
    "outcomes = ['Anxiety', 'Depression', 'ADHD', 'Behavior Problems']\n",
    "model_types = ['LogisticRegression', 'XGBoost', 'RandomForest', 'GradientBoost', 'NaiveBayes', 'LightGBM']\n",
    "\n",
    "# Best model per outcome based on performance\n",
    "out2best_model = {\n",
    "    'Anxiety': 'LogisticRegression',\n",
    "    'Depression': 'LogisticRegression',\n",
    "    'ADHD': 'GradientBoost',\n",
    "    'Behavior Problems': 'LogisticRegression'\n",
    "}\n",
    "\n",
    "# Configuration parameters\n",
    "CONFIG = {\n",
    "    'target_adjustment_factor': 0.5,  # Factor to adjust Y_target values (formerly hardcoded as 0.5)\n",
    "    'optimization_lambda': 1.0,       # Lambda parameter for optimization regularization\n",
    "    'random_seed': 42                 # Random seed for reproducibility\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "# STEP 1: MACHINE LEARNING PREDICTION MODEL\n",
    "#################################################\n",
    "\n",
    "def prepare_nsch_data():\n",
    "    \"\"\"\n",
    "    Load and prepare NSCH data for machine learning model training.\n",
    "    \"\"\"\n",
    "    print(\"Step 1.1: Loading and preparing NSCH data...\")\n",
    "    \n",
    "    # Load the NSCH data\n",
    "    NSCH = pd.read_csv('data/2021 NSCH_Topical_CAHMI_DRC.csv')\n",
    "    \n",
    "    # Create age group column\n",
    "    NSCH['SC_AGE_GROUP'] = pd.cut(NSCH['SC_AGE_YEARS'], \n",
    "                                  bins=[0, 4, 9, 13, 17], \n",
    "                                  labels=[1, 2, 3, 4])\n",
    "    NSCH = NSCH.dropna(subset=['SC_AGE_GROUP'])\n",
    "    NSCH['SC_AGE_GROUP'] = NSCH['SC_AGE_GROUP'].astype(int)\n",
    "    \n",
    "    # Keep only 14-17 years old children in the dataset\n",
    "    NSCH = NSCH[NSCH['SC_AGE_GROUP'] == 4]\n",
    "    \n",
    "    # Recategorize variables\n",
    "    NSCH['race7_21'] = NSCH['race7_21'].apply(lambda x: x if x in [1, 2, 3, 4] else 5)\n",
    "    NSCH['EmploymentSt_21'] = NSCH['EmploymentSt_21'].apply(lambda x: 1 if x in [1, 2] else 2)\n",
    "    NSCH['famstruct5_21'] = NSCH['famstruct5_21'].apply(lambda x: 4 if x == 5 else x)\n",
    "    NSCH['FamCount_21'] = NSCH['FamCount_21'].apply(lambda x: 1 if x == 1 else 2 if x in [2, 3, 4] else 3)\n",
    "    NSCH['ACEincome_21'] = NSCH['ACEincome_21'].apply(lambda x: 1 if x in [1, 2] else 2)\n",
    "    \n",
    "    # Exclude invalid data\n",
    "    for var in NSCH_ACS_variables:\n",
    "        NSCH = NSCH[NSCH[name2col[var]] != 99]\n",
    "        NSCH = NSCH[NSCH[name2col[var]] != 95]\n",
    "        \n",
    "    print(f'The number of entries in NSCH dataset: {NSCH.shape[0]}')\n",
    "    return NSCH\n",
    "\n",
    "def train_mental_health_models(NSCH):\n",
    "    \"\"\"\n",
    "    Train machine learning models to predict mental health outcomes.\n",
    "    \"\"\"\n",
    "    print(\"Step 1.2: Training mental health prediction models...\")\n",
    "    \n",
    "    variables = [name2col[var] for var in used_vars]\n",
    "    out2model_type2model = {}\n",
    "    out2X_train = {}\n",
    "    out2X_test = {}\n",
    "    out2y_train = {}\n",
    "    out2y_test = {}\n",
    "    \n",
    "    for out in outcomes:\n",
    "        print(f\"Training models for {out}\")\n",
    "        X_ori = NSCH[variables]\n",
    "        idxs_missing_x = X_ori.isin([99, 95, 'NaN']).any(axis=1)\n",
    "        \n",
    "        y_ori = NSCH[name2col[out]]\n",
    "        y_ori = y_ori.replace(2, 0)  # Convert 'no' (2) to 0\n",
    "        idxs_missing_y = y_ori.isin([99, 95])\n",
    "        \n",
    "        idxs_missing = idxs_missing_x | idxs_missing_y\n",
    "        X = X_ori[~idxs_missing]\n",
    "        y = y_ori[~idxs_missing]\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Optional: SMOTE for imbalanced data\n",
    "        # sm = SMOTE(random_state=42)\n",
    "        # X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "        \n",
    "        out2X_train[out] = X_train\n",
    "        out2X_test[out] = X_test\n",
    "        out2y_train[out] = y_train\n",
    "        out2y_test[out] = y_test\n",
    "        \n",
    "        for model_type in model_types:\n",
    "            print(f'  Training {model_type} for {out}')\n",
    "            if model_type == 'LogisticRegression':\n",
    "                model = LogisticRegression(max_iter=1000)\n",
    "            elif model_type == 'XGBoost':\n",
    "                model = XGBClassifier()\n",
    "            elif model_type == 'RandomForest':\n",
    "                model = RandomForestClassifier()\n",
    "            elif model_type == 'GradientBoost':\n",
    "                model = GradientBoostingClassifier()\n",
    "            elif model_type == 'NaiveBayes':\n",
    "                model = GaussianNB()\n",
    "            elif model_type == 'LightGBM':\n",
    "                model = lgb.LGBMClassifier()\n",
    "                \n",
    "            model.fit(X_train, y_train)\n",
    "            out2model_type2model.setdefault(out, {})[model_type] = model\n",
    "    \n",
    "    # Save the models\n",
    "    os.makedirs('new_model', exist_ok=True)\n",
    "    for out in out2model_type2model:\n",
    "        for model_type in out2model_type2model[out]:\n",
    "            model = out2model_type2model[out][model_type]\n",
    "            joblib.dump(model, f'new_model/{out}_{model_type}.pkl')\n",
    "    \n",
    "    # Save data splits\n",
    "    joblib.dump(out2X_train, 'new_model/out2X_train.pkl')\n",
    "    joblib.dump(out2X_test, 'new_model/out2X_test.pkl')\n",
    "    joblib.dump(out2y_train, 'new_model/out2y_train.pkl')\n",
    "    joblib.dump(out2y_test, 'new_model/out2y_test.pkl')\n",
    "    \n",
    "    return out2model_type2model, out2X_train, out2X_test, out2y_train, out2y_test\n",
    "\n",
    "def evaluate_models(out2model_type2model, out2X_test, out2y_test):\n",
    "    \"\"\"\n",
    "    Evaluate model performance using ROC curves and AUC scores.\n",
    "    \"\"\"\n",
    "    print(\"Step 1.3: Evaluating model performance...\")\n",
    "    \n",
    "    model2out2auc = {}\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.rcParams.update({'font.size': 12, 'figure.dpi': 300})\n",
    "    \n",
    "    for i, out in enumerate(outcomes):\n",
    "        plt.subplot(2, 2, i + 1)\n",
    "        X_test = out2X_test[out]\n",
    "        y_test = out2y_test[out]\n",
    "        \n",
    "        for model_type in model_types:\n",
    "            model = out2model_type2model[out][model_type]\n",
    "            y_pred = model.predict_proba(X_test)[:, 1]\n",
    "            fpr, tpr, thresholds = roc_curve(y_test, y_pred, pos_label=1)\n",
    "            auc = roc_auc_score(y_test, y_pred)\n",
    "            model2out2auc.setdefault(model_type, {})[out] = auc\n",
    "            \n",
    "            plt.plot(fpr, tpr, label=f\"{model_type} (AUC={auc:.3f})\", \n",
    "                     color=sns.color_palette()[model_types.index(model_type)])\n",
    "            \n",
    "        plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'{out}')\n",
    "        plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(hspace=0.3, wspace=0.15)\n",
    "    plt.suptitle('ROC Curves for Mental Health Prediction Models')\n",
    "    plt.savefig('results/model_roc_curves.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Save AUC scores\n",
    "    df = pd.DataFrame(model2out2auc)\n",
    "    df = df.round(3)\n",
    "    df.to_csv('results/model_auc.csv')\n",
    "    \n",
    "    return model2out2auc\n",
    "\n",
    "def calculate_estimated_probabilities(out2model_type2model):\n",
    "    \"\"\"\n",
    "    Calculate estimated probabilities for all demographic subgroups.\n",
    "    \"\"\"\n",
    "    print(\"Step 1.4: Calculating estimated probabilities for demographic subgroups...\")\n",
    "    \n",
    "    features = list(var2value2name.keys())\n",
    "    features_col = [name2col[feature] for feature in features]\n",
    "    \n",
    "    # Get the dimensions of the joint distribution\n",
    "    dimensions = tuple(len(var2value2name[feature]) for feature in features)\n",
    "    print(f'Dimensions: {dimensions}')\n",
    "    print(f'Total number of demographic subgroups: {np.prod(dimensions)}')\n",
    "    \n",
    "    # Calculate probabilities for each subgroup using the best model\n",
    "    estimated_prob = np.zeros(dimensions)\n",
    "    \n",
    "    for idx, values in enumerate(itertools.product(*[var2value2name[feature].keys() for feature in features])):\n",
    "        feature_dict = {feature_col: value for feature_col, value in zip(features_col, values)}\n",
    "        # Using Depression model as an example\n",
    "        prob = out2model_type2model['Depression']['LogisticRegression'].predict_proba(pd.DataFrame([feature_dict]))[0][1]\n",
    "        estimated_prob[np.unravel_index(idx, dimensions)] = prob\n",
    "    \n",
    "    return estimated_prob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "# STEP 2: ACS DATA COLLECTION (COUNTY LEVEL)\n",
    "#################################################\n",
    "\n",
    "def collect_acs_demographic_data():\n",
    "    \"\"\"\n",
    "    Collect demographic data from ACS for Washington state counties.\n",
    "    \"\"\"\n",
    "    print(\"Step 2.1: Collecting ACS demographic data for Washington counties...\")\n",
    "    \n",
    "    # Define variable lists for ACS data collection\n",
    "    age_sex_var = ['B01001_006E', 'B01001_030E']\n",
    "    age_sex_white_var = ['B01001H_006E', 'B01001H_021E']\n",
    "    age_sex_black_var = ['B01001B_006E', 'B01001B_021E']\n",
    "    age_sex_asian_var = ['B01001D_006E', 'B01001D_021E']\n",
    "    age_sex_hispanic_var = ['B01001I_006E', 'B01001I_021E']\n",
    "    \n",
    "    target_var = age_sex_var + age_sex_white_var + age_sex_black_var + age_sex_asian_var + age_sex_hispanic_var\n",
    "    target_var = ['NAME'] + target_var\n",
    "    \n",
    "    # Collect age, sex, and race data\n",
    "    age_sex_race = c.acs5.state_county(\n",
    "        fields=target_var,\n",
    "        state_fips=us.states.WA.fips,\n",
    "        county_fips=\"*\",\n",
    "        year=2022\n",
    "    )\n",
    "    age_sex_race_df = pd.DataFrame(age_sex_race)\n",
    "    \n",
    "    # Calculate \"other\" race populations\n",
    "    age_sex_race_df['B01001O_006E'] = (\n",
    "        age_sex_race_df['B01001_006E'] - \n",
    "        age_sex_race_df['B01001H_006E'] - \n",
    "        age_sex_race_df['B01001B_006E'] - \n",
    "        age_sex_race_df['B01001D_006E'] - \n",
    "        age_sex_race_df['B01001I_006E']\n",
    "    )\n",
    "    age_sex_race_df['B01001O_021E'] = (\n",
    "        age_sex_race_df['B01001_030E'] - \n",
    "        age_sex_race_df['B01001H_021E'] - \n",
    "        age_sex_race_df['B01001B_021E'] - \n",
    "        age_sex_race_df['B01001D_021E'] - \n",
    "        age_sex_race_df['B01001I_021E']\n",
    "    )\n",
    "    \n",
    "    # Add total population column\n",
    "    age_sex_race_df['Total'] = age_sex_race_df['B01001_006E'] + age_sex_race_df['B01001_030E']\n",
    "    \n",
    "    # Collect employment and insurance data\n",
    "    employ_insurance_var = ['NAME', 'B27011_002E', 'B27011_004E', 'B27011_007E', 'B27011_009E', 'B27011_012E']\n",
    "    employ_insurance = c.acs5.state_county(\n",
    "        fields=employ_insurance_var, \n",
    "        state_fips=us.states.WA.fips, \n",
    "        county_fips=\"*\",\n",
    "        year=2022\n",
    "    )\n",
    "    employ_insurance_df = pd.DataFrame(employ_insurance)\n",
    "    \n",
    "    # Collect language data\n",
    "    language_var = ['NAME', 'B10054_001E', 'B10054_013E']\n",
    "    language = c.acs5.state_county(\n",
    "        fields=language_var, \n",
    "        state_fips=us.states.WA.fips, \n",
    "        county_fips=\"*\",\n",
    "        year=2022\n",
    "    )\n",
    "    language_df = pd.DataFrame(language)\n",
    "    language_df['B10054_002E'] = language_df['B10054_001E'] - language_df['B10054_013E']\n",
    "    language_df.loc[language_df['B10054_002E'] < 0, 'B10054_002E'] = 0\n",
    "    language_df = language_df.rename(columns={'B10054_013E': 'B10054_003E'})\n",
    "    \n",
    "    # Collect education data\n",
    "    education_var = ['NAME', 'B07009_001E', 'B07009_002E', 'B07009_003E', 'B07009_004E', 'B07009_005E', 'B07009_006E']\n",
    "    education = c.acs5.state_county(\n",
    "        fields=education_var, \n",
    "        state_fips=us.states.WA.fips, \n",
    "        county_fips=\"*\",\n",
    "        year=2022\n",
    "    )\n",
    "    education_df = pd.DataFrame(education)\n",
    "    education_df['B07009_005E'] = education_df['B07009_005E'] + education_df['B07009_006E']\n",
    "    education_df = education_df.drop('B07009_006E', axis=1)\n",
    "    \n",
    "    # Collect income data\n",
    "    income_var = ['NAME', 'B17004_001E', 'B17004_002E', 'B17004_011E']\n",
    "    income = c.acs5.state_county(\n",
    "        fields=income_var, \n",
    "        state_fips=us.states.WA.fips, \n",
    "        county_fips=\"*\",\n",
    "        year=2022\n",
    "    )\n",
    "    income_df = pd.DataFrame(income)\n",
    "    \n",
    "    # Collect family structure data\n",
    "    family_structure_var = ['NAME', 'B09005_001E', 'B09005_002E', 'B09005_003E', 'B09005_004E', 'B09005_005E']\n",
    "    family_structure = c.acs5.state_county(\n",
    "        fields=family_structure_var, \n",
    "        state_fips=us.states.WA.fips, \n",
    "        county_fips=\"*\",\n",
    "        year=2022\n",
    "    )\n",
    "    family_structure_df = pd.DataFrame(family_structure)\n",
    "    family_structure_df['B09005_004E'] = family_structure_df['B09005_004E'] + family_structure_df['B09005_005E']\n",
    "    family_structure_df = family_structure_df.drop('B09005_005E', axis=1)\n",
    "    family_structure_df['B09005_005E'] = (\n",
    "        family_structure_df['B09005_001E'] - \n",
    "        family_structure_df['B09005_002E'] - \n",
    "        family_structure_df['B09005_003E'] - \n",
    "        family_structure_df['B09005_004E']\n",
    "    )\n",
    "    family_structure_df['B09005_005E'] = family_structure_df['B09005_005E'].apply(lambda x: 0 if x < 0 else x)\n",
    "    \n",
    "    return (age_sex_race_df, employ_insurance_df, language_df, education_df, income_df, family_structure_df)\n",
    "\n",
    "def calculate_marginal_distributions(age_sex_race_df, employ_insurance_df, language_df, education_df, income_df, family_structure_df):\n",
    "    \"\"\"\n",
    "    Calculate marginal distributions from ACS data for each county.\n",
    "    \"\"\"\n",
    "    print(\"Step 2.2: Calculating marginal distributions from ACS data...\")\n",
    "    \n",
    "    # Create joint distribution for race, sex, age\n",
    "    name2marginal_race_sex_age = {}\n",
    "    for idx, row in age_sex_race_df.iterrows():\n",
    "        marginal = {}\n",
    "        for values in itertools.product(*[var2value2name[feature].keys() for feature in used_vars[0:2]]):\n",
    "            row_name = 'B01001'\n",
    "            row_name += {1: 'I_', 2: 'H_', 3: 'B_', 4: 'D_', 5: 'O_'}[values[0]]\n",
    "            if values[1] == 1:\n",
    "                row_name += '006E'  # Male 14-17\n",
    "            elif values[1] == 2:\n",
    "                row_name += '021E'  # Female 14-17\n",
    "            \n",
    "            marginal[values] = row[row_name]/row['Total'] if row['Total'] > 0 else 0\n",
    "        name2marginal_race_sex_age[row['NAME']] = marginal\n",
    "\n",
    "    # Create joint distribution for insurance and employment\n",
    "    name2marginal_ins_emp = {}\n",
    "    for idx, row in employ_insurance_df.iterrows():\n",
    "        marginal = {}\n",
    "        total = row['B27011_002E']\n",
    "        if total > 0:\n",
    "            marginal[(1, 1)] = row['B27011_004E'] / total  # Employed with insurance\n",
    "            marginal[(1, 2)] = row['B27011_007E'] / total  # Employed without insurance\n",
    "            marginal[(2, 1)] = row['B27011_009E'] / total  # Unemployed with insurance\n",
    "            marginal[(2, 2)] = row['B27011_012E'] / total  # Unemployed without insurance\n",
    "        else:\n",
    "            marginal[(1, 1)] = 0\n",
    "            marginal[(1, 2)] = 0\n",
    "            marginal[(2, 1)] = 0\n",
    "            marginal[(2, 2)] = 0\n",
    "        name2marginal_ins_emp[row['NAME']] = marginal\n",
    "\n",
    "    # Create marginal distribution for education\n",
    "    name2marginal_edu = {}\n",
    "    for idx, row in education_df.iterrows():\n",
    "        marginal = {}\n",
    "        total = row['B07009_001E']\n",
    "        if total > 0:\n",
    "            marginal[(1,)] = row['B07009_002E'] / total  # Less than high school\n",
    "            marginal[(2,)] = row['B07009_003E'] / total  # High school or GED\n",
    "            marginal[(3,)] = row['B07009_004E'] / total  # Some college or technical school\n",
    "            marginal[(4,)] = row['B07009_005E'] / total  # College degree or higher\n",
    "        else:\n",
    "            marginal[(1,)] = 0\n",
    "            marginal[(2,)] = 0\n",
    "            marginal[(3,)] = 0\n",
    "            marginal[(4,)] = 0\n",
    "        name2marginal_edu[row['NAME']] = marginal\n",
    "\n",
    "    # Create marginal distribution for language\n",
    "    name2marginal_lang = {}\n",
    "    for idx, row in language_df.iterrows():\n",
    "        marginal = {}\n",
    "        total = row['B10054_001E']\n",
    "        if total > 0:\n",
    "            marginal[(1,)] = row['B10054_002E'] / total  # Primarily English\n",
    "            marginal[(2,)] = row['B10054_003E'] / total  # Others\n",
    "        else:\n",
    "            marginal[(1,)] = 0\n",
    "            marginal[(2,)] = 0\n",
    "        name2marginal_lang[row['NAME']] = marginal\n",
    "\n",
    "    # Create marginal distribution for family structure\n",
    "    name2marginal_fam_struct = {}\n",
    "    for idx, row in family_structure_df.iterrows():\n",
    "        marginal = {}\n",
    "        total = row['B09005_001E']\n",
    "        if total > 0:\n",
    "            marginal[(1,)] = row['B09005_002E'] / total  # Two parents, currently married\n",
    "            marginal[(2,)] = row['B09005_003E'] / total  # Two parents, not currently married\n",
    "            marginal[(3,)] = row['B09005_004E'] / total  # Single parent\n",
    "            marginal[(4,)] = row['B09005_005E'] / total  # Other family type\n",
    "        else:\n",
    "            marginal[(1,)] = 0\n",
    "            marginal[(2,)] = 0\n",
    "            marginal[(3,)] = 0\n",
    "            marginal[(4,)] = 0\n",
    "        name2marginal_fam_struct[row['NAME']] = marginal\n",
    "\n",
    "    # Create marginal distribution for family income\n",
    "    name2marginal_income = {}\n",
    "    for idx, row in income_df.iterrows():\n",
    "        marginal = {}\n",
    "        total = row['B17004_001E']\n",
    "        if total > 0:\n",
    "            marginal[(1,)] = row['B17004_011E'] / total  # Below poverty line\n",
    "            marginal[(2,)] = row['B17004_002E'] / total  # Above poverty line\n",
    "        else:\n",
    "            marginal[(1,)] = 0\n",
    "            marginal[(2,)] = 0\n",
    "        name2marginal_income[row['NAME']] = marginal\n",
    "\n",
    "    return (name2marginal_race_sex_age, name2marginal_ins_emp, name2marginal_edu, \n",
    "            name2marginal_lang, name2marginal_fam_struct, name2marginal_income)\n",
    "\n",
    "def load_hys_data():\n",
    "    \"\"\"\n",
    "    Load Healthy Youth Survey (HYS) data for depression among 10th graders.\n",
    "    \"\"\"\n",
    "    print(\"Step 2.3: Loading Healthy Youth Survey (HYS) data...\")\n",
    "    \n",
    "    # Import HYS data\n",
    "    hys_depression_10th = pd.read_csv('data/HYS/depression_10th_grade_2018.csv')\n",
    "    \n",
    "    # Format county names to match ACS data\n",
    "    hys_depression_10th['County'] = hys_depression_10th['County'] + ' County, Washington'\n",
    "    \n",
    "    return hys_depression_10th\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "# STEP 3: OPTIMIZATION MODEL FOR JOINT DISTRIBUTION\n",
    "#################################################\n",
    "\n",
    "def optimize_demographic_proportions(Y_target, dimensions, estimated_prob, lambda_param, \n",
    "                                     marginal_race_sex_age, marginal_ins_emp, marginal_edu, \n",
    "                                     marginal_lang, marginal_fam_struct, marginal_income):\n",
    "    \"\"\"\n",
    "    Optimize joint demographic distribution using marginal constraints from ACS.\n",
    "    \"\"\"\n",
    "    model = gp.Model(\"DemographicProportions\")\n",
    "    \n",
    "    # Create variables for each element in the joint distribution tensor\n",
    "    X_d = model.addVars(*dimensions, vtype=GRB.CONTINUOUS, name=\"X_d\")\n",
    "    \n",
    "    # Objective: minimize squared difference between estimated and target prevalence with regularization\n",
    "    objective = (gp.quicksum(X_d[i] * estimated_prob[i] for i in np.ndindex(dimensions)) - Y_target) ** 2\n",
    "    penalty = lambda_param * gp.quicksum(X_d[i] ** 2 for i in np.ndindex(dimensions))\n",
    "    model.setObjective(objective + penalty, GRB.MINIMIZE)\n",
    "\n",
    "    # Add constraints for race-sex-age joint distribution\n",
    "    idxs = [0, 1]\n",
    "    feature_list = [used_vars[idx] for idx in idxs]\n",
    "    for values in itertools.product(*[var2value2name[feature].keys() for feature in feature_list]):\n",
    "        category_sum = gp.quicksum(\n",
    "            X_d[i] for i in np.ndindex(dimensions) \n",
    "            if all(i[idx] == value - 1 for idx, value in zip(idxs, values))\n",
    "        )\n",
    "        model.addConstr(category_sum >= marginal_race_sex_age[values], \n",
    "                        name=f\"marginal_race_sex_age_{values}\")\n",
    "\n",
    "    # Add constraints for education\n",
    "    idxs = [2]\n",
    "    feature_list = [used_vars[idx] for idx in idxs]\n",
    "    for values in itertools.product(*[var2value2name[feature].keys() for feature in feature_list]):\n",
    "        category_sum = gp.quicksum(\n",
    "            X_d[i] for i in np.ndindex(dimensions) \n",
    "            if all(i[idx] == value - 1 for idx, value in zip(idxs, values))\n",
    "        )\n",
    "        model.addConstr(category_sum >= marginal_edu[values], \n",
    "                        name=f\"marginal_edu_{values}\")\n",
    "\n",
    "    # Add constraints for language\n",
    "    idxs = [3]\n",
    "    feature_list = [used_vars[idx] for idx in idxs]\n",
    "    for values in itertools.product(*[var2value2name[feature].keys() for feature in feature_list]):\n",
    "        category_sum = gp.quicksum(\n",
    "            X_d[i] for i in np.ndindex(dimensions) \n",
    "            if all(i[idx] == value - 1 for idx, value in zip(idxs, values))\n",
    "        )\n",
    "        model.addConstr(category_sum >= marginal_lang[values], \n",
    "                       name=f\"marginal_lang_{values}\")\n",
    "\n",
    "    # Add constraints for employment and insurance\n",
    "    idxs = [4, 5]\n",
    "    feature_list = [used_vars[idx] for idx in idxs]\n",
    "    for values in itertools.product(*[var2value2name[feature].keys() for feature in feature_list]):\n",
    "        category_sum = gp.quicksum(\n",
    "            X_d[i] for i in np.ndindex(dimensions) \n",
    "            if all(i[idx] == value - 1 for idx, value in zip(idxs, values))\n",
    "        )\n",
    "        model.addConstr(category_sum >= marginal_ins_emp[values], \n",
    "                       name=f\"marginal_ins_emp_{values}\")\n",
    "\n",
    "    # Add constraints for family income\n",
    "    idxs = [6]\n",
    "    feature_list = [used_vars[idx] for idx in idxs]\n",
    "    for values in itertools.product(*[var2value2name[feature].keys() for feature in feature_list]):\n",
    "        category_sum = gp.quicksum(\n",
    "            X_d[i] for i in np.ndindex(dimensions) \n",
    "            if all(i[idx] == value - 1 for idx, value in zip(idxs, values))\n",
    "        )\n",
    "        model.addConstr(category_sum >= marginal_income[values], \n",
    "                       name=f\"marginal_income_{values}\")\n",
    "\n",
    "    # Add constraints for family structure\n",
    "    idxs = [7]\n",
    "    feature_list = [used_vars[idx] for idx in idxs]\n",
    "    for values in itertools.product(*[var2value2name[feature].keys() for feature in feature_list]):\n",
    "        category_sum = gp.quicksum(\n",
    "            X_d[i] for i in np.ndindex(dimensions) \n",
    "            if all(i[idx] == value - 1 for idx, value in zip(idxs, values))\n",
    "        )\n",
    "        model.addConstr(category_sum >= marginal_fam_struct[values], \n",
    "                       name=f\"marginal_fam_struct_{values}\")\n",
    "\n",
    "    # Normalization constraint: probabilities sum to 1\n",
    "    model.addConstr(gp.quicksum(X_d[i] for i in np.ndindex(dimensions)) == 1, \n",
    "                   name=\"normalization\")\n",
    "\n",
    "    # Optimize the model\n",
    "    model.optimize()\n",
    "\n",
    "    # Retrieve the optimized proportions\n",
    "    optimized_proportions = np.zeros(dimensions)\n",
    "    for i in np.ndindex(dimensions):\n",
    "        optimized_proportions[i] = X_d[i].X\n",
    "\n",
    "    return optimized_proportions\n",
    "\n",
    "def run_optimization_for_all_counties(age_sex_race_df, estimated_prob, marginal_distributions, hys_depression_10th):\n",
    "    \"\"\"\n",
    "    Run optimization for all counties to estimate depression prevalence.\n",
    "    \"\"\"\n",
    "    print(\"Step 3.1: Running optimization for all counties...\")\n",
    "    \n",
    "    # Unpack marginal distributions\n",
    "    (name2marginal_race_sex_age, name2marginal_ins_emp, name2marginal_edu, \n",
    "     name2marginal_lang, name2marginal_fam_struct, name2marginal_income) = marginal_distributions\n",
    "    \n",
    "    # Prepare HYS data\n",
    "    depression_10th = pd.DataFrame(age_sex_race_df['NAME'].values, columns=['County'])\n",
    "    depression_10th = depression_10th.merge(hys_depression_10th[['County', 'Percentage']], on='County', how='left')\n",
    "    depression_10th['Percentage'] = depression_10th['Percentage'].fillna('40.0%')\n",
    "    depression_10th['Percentage'] = depression_10th['Percentage'].replace('%', '', regex=True).astype(float)/100\n",
    "    depression_10th = depression_10th[['County', 'Percentage']]\n",
    "    \n",
    "    # Set optimization parameters\n",
    "    lambda_param = CONFIG['optimization_lambda']\n",
    "    dimensions = estimated_prob.shape\n",
    "    \n",
    "    # Disable Gurobi output\n",
    "    gp.setParam('OutputFlag', 0)\n",
    "    \n",
    "    # Create directory for results\n",
    "    os.makedirs('results/proportions_depression_14_17', exist_ok=True)\n",
    "    \n",
    "    # Run optimization for each county\n",
    "    for idx, row in age_sex_race_df.iterrows():\n",
    "        county_name = row['NAME']\n",
    "        print(f'Solving for {county_name}')\n",
    "        \n",
    "        # Get target depression rate from HYS data (using config parameter instead of hardcoded 0.5)\n",
    "        Y_target = depression_10th[depression_10th['County'] == county_name]['Percentage'].values[0] * CONFIG['target_adjustment_factor']\n",
    "        \n",
    "        # Get marginal distributions for this county\n",
    "        marginal_race_sex_age = name2marginal_race_sex_age[county_name]\n",
    "        marginal_ins_emp = name2marginal_ins_emp[county_name]\n",
    "        marginal_edu = name2marginal_edu[county_name]\n",
    "        marginal_lang = name2marginal_lang[county_name]\n",
    "        marginal_fam_struct = name2marginal_fam_struct[county_name]\n",
    "        marginal_income = name2marginal_income[county_name]\n",
    "        \n",
    "        # Run optimization\n",
    "        optimized_proportions = optimize_demographic_proportions(\n",
    "            Y_target, dimensions, estimated_prob, lambda_param,\n",
    "            marginal_race_sex_age, marginal_ins_emp, marginal_edu,\n",
    "            marginal_lang, marginal_fam_struct, marginal_income\n",
    "        )\n",
    "        \n",
    "        # Save the optimized proportions\n",
    "        with open(f'results/proportions_depression_14_17/{county_name}.pkl', 'wb') as f:\n",
    "            pickle.dump(optimized_proportions, f)\n",
    "    \n",
    "    return depression_10th\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#################################################\n",
    "# STEP 4: MAP VISUALIZATION\n",
    "#################################################\n",
    "\n",
    "def calculate_estimated_prevalence(depression_10th, estimated_prob):\n",
    "    \"\"\"\n",
    "    Calculate estimated depression prevalence using optimized demographic proportions.\n",
    "    \"\"\"\n",
    "    print(\"Step 4.1: Calculating estimated depression prevalence for each county...\")\n",
    "    \n",
    "    # Add column for estimated prevalence\n",
    "    depression_10th['Estimated'] = np.nan\n",
    "    \n",
    "    # Iterate over counties and calculate estimated prevalence\n",
    "    for idx, row in depression_10th.iterrows():\n",
    "        county_name = row['County']\n",
    "        \n",
    "        # Load optimized proportions\n",
    "        with open(f'results/proportions_depression_14_17/{county_name}.pkl', 'rb') as f:\n",
    "            optimized_proportions = pickle.load(f)\n",
    "        \n",
    "        print(f'Estimating for {county_name}, Total proportion: {np.sum(optimized_proportions)}')\n",
    "        \n",
    "        # Calculate estimated prevalence as weighted sum\n",
    "        est_prev = np.sum(optimized_proportions * estimated_prob)\n",
    "        depression_10th.at[idx, 'Estimated'] = est_prev\n",
    "    \n",
    "    # Clean up county names for display\n",
    "    depression_10th['County'] = depression_10th['County'].str.replace(' County, Washington', '')\n",
    "    \n",
    "    # Adjust reported rates to match original code (using config parameter instead of hardcoded 0.5)\n",
    "    depression_10th['Percentage'] = depression_10th['Percentage'] * CONFIG['target_adjustment_factor']\n",
    "    \n",
    "    return depression_10th\n",
    "\n",
    "def evaluate_model_performance(depression_10th):\n",
    "    \"\"\"\n",
    "    Evaluate model performance by comparing estimated vs. reported prevalence.\n",
    "    \"\"\"\n",
    "    print(\"Step 4.2: Evaluating model performance...\")\n",
    "    \n",
    "    # Calculate Mean Absolute Percentage Error (MAPE)\n",
    "    mape = np.mean(np.abs(depression_10th['Percentage'] - depression_10th['Estimated']) / depression_10th['Percentage'])\n",
    "    print(f'MAPE: {mape:.4f}')\n",
    "    \n",
    "    # Calculate Weighted Absolute Percentage Error (WAPE)\n",
    "    wape = np.sum(np.abs(depression_10th['Percentage'] - depression_10th['Estimated'])) / np.sum(depression_10th['Percentage'])\n",
    "    print(f'WAPE: {wape:.4f}')\n",
    "    \n",
    "    # Calculate Mean Percentage Error (MPE)\n",
    "    mpe = np.mean((depression_10th['Percentage'] - depression_10th['Estimated']) / depression_10th['Percentage'])\n",
    "    print(f'MPE: {mpe:.4f}')\n",
    "    \n",
    "    return mape, wape, mpe\n",
    "\n",
    "def create_bar_chart(depression_10th):\n",
    "    \"\"\"\n",
    "    Create bar chart comparing estimated vs. reported depression rates.\n",
    "    \"\"\"\n",
    "    print(\"Step 4.3: Creating bar chart comparison...\")\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.rcParams.update({'font.size': 12, 'figure.dpi': 300})\n",
    "    \n",
    "    # Create side-by-side bars\n",
    "    barWidth = 0.35\n",
    "    r1 = np.arange(len(depression_10th))\n",
    "    r2 = [x + barWidth for x in r1]\n",
    "    \n",
    "    plt.bar(r1, depression_10th['Percentage'], color='beige', width=barWidth, \n",
    "            edgecolor='grey', label='Adjusted HYS', alpha=0.7)\n",
    "    plt.bar(r2, depression_10th['Estimated'], color='plum', width=barWidth, \n",
    "            edgecolor='grey', label='Model Estimation', alpha=0.7)\n",
    "    \n",
    "    # Add statewide average reference line\n",
    "    plt.axhline(y=0.2, color='r', linestyle='--', label='Statewide from HYS')\n",
    "    \n",
    "    # Format chart\n",
    "    plt.xlabel('County', fontweight='bold')\n",
    "    plt.xticks([r + barWidth/2 for r in range(len(depression_10th))], \n",
    "              depression_10th['County'], rotation=90)\n",
    "    plt.ylabel('Depression Rate')\n",
    "    plt.title('Comparisons of Reported and Estimated Depression Rates for Children Aged 14-17')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the chart\n",
    "    plt.savefig('results/depression_comparison_chart.png')\n",
    "    plt.close()\n",
    "\n",
    "def create_choropleth_map(depression_10th):\n",
    "    \"\"\"\n",
    "    Create choropleth map of estimated depression prevalence.\n",
    "    \"\"\"\n",
    "    print(\"Step 4.4: Creating choropleth map...\")\n",
    "    \n",
    "    # Access shapefile of Washington state counties\n",
    "    wa_tract = gpd.read_file(\"https://www2.census.gov/geo/tiger/TIGER2019/COUNTY/tl_2019_us_county.zip\")\n",
    "    \n",
    "    # Reproject shapefile to UTM Zone 10N (EPSG: 32610) for Washington state\n",
    "    wa_tract = wa_tract.to_crs(epsg=32610)\n",
    "    \n",
    "    # Keep only Washington state counties\n",
    "    wa_tract = wa_tract[wa_tract['STATEFP'] == '53']\n",
    "    \n",
    "    # Prepare county data\n",
    "    wa_df = depression_10th.rename(columns={'County': 'NAME'})\n",
    "    \n",
    "    # Merge county data with geodataframe\n",
    "    wa_merge = wa_tract.merge(wa_df, on=\"NAME\", how=\"left\")\n",
    "    gplt = wa_merge[[\"STATEFP\", \"COUNTYFP\", \"NAME\", \"geometry\", \"Percentage\", \"Estimated\"]]\n",
    "    \n",
    "    # Create map\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(20, 10), dpi=300)\n",
    "    gplt.plot(column=\"Estimated\", ax=ax, cmap='OrRd', legend=True)\n",
    "    \n",
    "    # Add county boundaries\n",
    "    gplt.boundary.plot(ax=ax, color='black', linewidth=1)\n",
    "    \n",
    "    # Add county labels\n",
    "    for x, y, label in zip(gplt.geometry.centroid.x, gplt.geometry.centroid.y, gplt['NAME']):\n",
    "        plt.text(x, y, label, fontsize=8, ha='center', color='black')\n",
    "    \n",
    "    # Set title\n",
    "    ax.set_title('Estimated Prevalence of Depression in Washington with Children Aged 14-17', \n",
    "                fontdict={'fontsize': '25', 'fontweight': '3'})\n",
    "    \n",
    "    # Save map\n",
    "    plt.savefig('results/depression_prevalence_map.png')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Mental Health Prevalence Analysis for Washington State...\n",
      "Using target adjustment factor: 0.5\n",
      "Using optimization lambda: 1.0\n",
      "\n",
      "===== STEP 1: MACHINE LEARNING PREDICTION MODEL =====\n",
      "Step 1.1: Loading and preparing NSCH data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m0/sm_vmzq13717njnwqkq0l5l80000gn/T/ipykernel_44022/261562639.py:12: DtypeWarning: Columns (35) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  NSCH = pd.read_csv('data/2021 NSCH_Topical_CAHMI_DRC.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of entries in NSCH dataset: 10944\n",
      "Step 1.2: Training mental health prediction models...\n",
      "Training models for Anxiety\n",
      "  Training LogisticRegression for Anxiety\n",
      "  Training XGBoost for Anxiety\n",
      "  Training RandomForest for Anxiety\n",
      "  Training GradientBoost for Anxiety\n",
      "  Training NaiveBayes for Anxiety\n",
      "  Training LightGBM for Anxiety\n",
      "[LightGBM] [Info] Number of positive: 2065, number of negative: 6651\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000651 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 31\n",
      "[LightGBM] [Info] Number of data points in the train set: 8716, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.236921 -> initscore=-1.169637\n",
      "[LightGBM] [Info] Start training from score -1.169637\n",
      "Training models for Depression\n",
      "  Training LogisticRegression for Depression\n",
      "  Training XGBoost for Depression\n",
      "  Training RandomForest for Depression\n",
      "  Training GradientBoost for Depression\n",
      "  Training NaiveBayes for Depression\n",
      "  Training LightGBM for Depression\n",
      "[LightGBM] [Info] Number of positive: 1376, number of negative: 7349\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000278 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 31\n",
      "[LightGBM] [Info] Number of data points in the train set: 8725, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.157708 -> initscore=-1.675384\n",
      "[LightGBM] [Info] Start training from score -1.675384\n",
      "Training models for ADHD\n",
      "  Training LogisticRegression for ADHD\n",
      "  Training XGBoost for ADHD\n",
      "  Training RandomForest for ADHD\n",
      "  Training GradientBoost for ADHD\n",
      "  Training NaiveBayes for ADHD\n",
      "  Training LightGBM for ADHD\n",
      "[LightGBM] [Info] Number of positive: 1563, number of negative: 7152\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000300 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 31\n",
      "[LightGBM] [Info] Number of data points in the train set: 8715, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.179346 -> initscore=-1.520785\n",
      "[LightGBM] [Info] Start training from score -1.520785\n",
      "Training models for Behavior Problems\n",
      "  Training LogisticRegression for Behavior Problems\n",
      "  Training XGBoost for Behavior Problems\n",
      "  Training RandomForest for Behavior Problems\n",
      "  Training GradientBoost for Behavior Problems\n",
      "  Training NaiveBayes for Behavior Problems\n",
      "  Training LightGBM for Behavior Problems\n",
      "[LightGBM] [Info] Number of positive: 938, number of negative: 7790\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000242 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 31\n",
      "[LightGBM] [Info] Number of data points in the train set: 8728, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.107470 -> initscore=-2.116846\n",
      "[LightGBM] [Info] Start training from score -2.116846\n",
      "Step 1.3: Evaluating model performance...\n",
      "Step 1.4: Calculating estimated probabilities for demographic subgroups...\n",
      "Dimensions: (5, 2, 4, 2, 2, 2, 2, 4)\n",
      "Total number of demographic subgroups: 2560\n",
      "\n",
      "===== STEP 2: ACS DATA COLLECTION =====\n",
      "Step 2.1: Collecting ACS demographic data for Washington counties...\n",
      "Step 2.2: Calculating marginal distributions from ACS data...\n",
      "Step 2.3: Loading Healthy Youth Survey (HYS) data...\n",
      "\n",
      "===== STEP 3: OPTIMIZATION MODEL =====\n",
      "Step 3.1: Running optimization for all counties...\n",
      "Solving for Adams County, Washington\n",
      "Solving for Asotin County, Washington\n",
      "Solving for Benton County, Washington\n",
      "Solving for Chelan County, Washington\n",
      "Solving for Clallam County, Washington\n",
      "Solving for Clark County, Washington\n",
      "Solving for Columbia County, Washington\n",
      "Solving for Cowlitz County, Washington\n",
      "Solving for Douglas County, Washington\n",
      "Solving for Ferry County, Washington\n",
      "Solving for Franklin County, Washington\n",
      "Solving for Garfield County, Washington\n",
      "Solving for Grant County, Washington\n",
      "Solving for Grays Harbor County, Washington\n",
      "Solving for Island County, Washington\n",
      "Solving for Jefferson County, Washington\n",
      "Solving for King County, Washington\n",
      "Solving for Kitsap County, Washington\n",
      "Solving for Kittitas County, Washington\n",
      "Solving for Klickitat County, Washington\n",
      "Solving for Lewis County, Washington\n",
      "Solving for Lincoln County, Washington\n",
      "Solving for Mason County, Washington\n",
      "Solving for Okanogan County, Washington\n",
      "Solving for Pacific County, Washington\n",
      "Solving for Pend Oreille County, Washington\n",
      "Solving for Pierce County, Washington\n",
      "Solving for San Juan County, Washington\n",
      "Solving for Skagit County, Washington\n",
      "Solving for Skamania County, Washington\n",
      "Solving for Snohomish County, Washington\n",
      "Solving for Spokane County, Washington\n",
      "Solving for Stevens County, Washington\n",
      "Solving for Thurston County, Washington\n",
      "Solving for Wahkiakum County, Washington\n",
      "Solving for Walla Walla County, Washington\n",
      "Solving for Whatcom County, Washington\n",
      "Solving for Whitman County, Washington\n",
      "Solving for Yakima County, Washington\n",
      "\n",
      "===== STEP 4: MAP VISUALIZATION =====\n",
      "Step 4.1: Calculating estimated depression prevalence for each county...\n",
      "Estimating for Adams County, Washington, Total proportion: 1.000000000000284\n",
      "Estimating for Asotin County, Washington, Total proportion: 1.0000000000000957\n",
      "Estimating for Benton County, Washington, Total proportion: 1.0000000000000706\n",
      "Estimating for Chelan County, Washington, Total proportion: 0.9999999999999251\n",
      "Estimating for Clallam County, Washington, Total proportion: 1.000000000002139\n",
      "Estimating for Clark County, Washington, Total proportion: 0.9999999999998986\n",
      "Estimating for Columbia County, Washington, Total proportion: 1.000000000050159\n",
      "Estimating for Cowlitz County, Washington, Total proportion: 0.9999999999998426\n",
      "Estimating for Douglas County, Washington, Total proportion: 1.0000000000000315\n",
      "Estimating for Ferry County, Washington, Total proportion: 0.9999999999998161\n",
      "Estimating for Franklin County, Washington, Total proportion: 0.9999999999976494\n",
      "Estimating for Garfield County, Washington, Total proportion: 0.9999999998400193\n",
      "Estimating for Grant County, Washington, Total proportion: 0.9999999999998795\n",
      "Estimating for Grays Harbor County, Washington, Total proportion: 1.000000000000205\n",
      "Estimating for Island County, Washington, Total proportion: 1.0000000000002507\n",
      "Estimating for Jefferson County, Washington, Total proportion: 1.0000000000044151\n",
      "Estimating for King County, Washington, Total proportion: 0.9999999999998748\n",
      "Estimating for Kitsap County, Washington, Total proportion: 1.0000000000011486\n",
      "Estimating for Kittitas County, Washington, Total proportion: 0.9999999999998503\n",
      "Estimating for Klickitat County, Washington, Total proportion: 0.9999999999992343\n",
      "Estimating for Lewis County, Washington, Total proportion: 0.9999999999999408\n",
      "Estimating for Lincoln County, Washington, Total proportion: 1.0000000000012863\n",
      "Estimating for Mason County, Washington, Total proportion: 0.9999999999993853\n",
      "Estimating for Okanogan County, Washington, Total proportion: 0.9999999999999415\n",
      "Estimating for Pacific County, Washington, Total proportion: 1.0000000000003237\n",
      "Estimating for Pend Oreille County, Washington, Total proportion: 0.999999999999828\n",
      "Estimating for Pierce County, Washington, Total proportion: 0.9999999999998381\n",
      "Estimating for San Juan County, Washington, Total proportion: 0.9999999999893041\n",
      "Estimating for Skagit County, Washington, Total proportion: 1.0000000000000437\n",
      "Estimating for Skamania County, Washington, Total proportion: 1.000000000003321\n",
      "Estimating for Snohomish County, Washington, Total proportion: 1.000000000000045\n",
      "Estimating for Spokane County, Washington, Total proportion: 0.9999999999926508\n",
      "Estimating for Stevens County, Washington, Total proportion: 0.9999999999998538\n",
      "Estimating for Thurston County, Washington, Total proportion: 1.000000000000423\n",
      "Estimating for Wahkiakum County, Washington, Total proportion: 0.9999999999975704\n",
      "Estimating for Walla Walla County, Washington, Total proportion: 0.9999999999999568\n",
      "Estimating for Whatcom County, Washington, Total proportion: 0.9999999999998839\n",
      "Estimating for Whitman County, Washington, Total proportion: 0.9999999999979932\n",
      "Estimating for Yakima County, Washington, Total proportion: 1.0000000000000329\n",
      "Step 4.2: Evaluating model performance...\n",
      "MAPE: 0.2666\n",
      "WAPE: 0.2740\n",
      "MPE: 0.2666\n",
      "Step 4.3: Creating bar chart comparison...\n",
      "Step 4.4: Creating choropleth map...\n",
      "\n",
      "Analysis complete! Results saved to output files.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function to run the entire analysis pipeline.\n",
    "    \"\"\"\n",
    "    print(\"Starting Mental Health Prevalence Analysis for Washington State...\")\n",
    "    print(f\"Using target adjustment factor: {CONFIG['target_adjustment_factor']}\")\n",
    "    print(f\"Using optimization lambda: {CONFIG['optimization_lambda']}\")\n",
    "    \n",
    "    # Step 1: Machine Learning Prediction Model\n",
    "    print(\"\\n===== STEP 1: MACHINE LEARNING PREDICTION MODEL =====\")\n",
    "    \n",
    "    # Step 1.1-1.2: Prepare data and train models\n",
    "    NSCH = prepare_nsch_data()\n",
    "    out2model_type2model, out2X_train, out2X_test, out2y_train, out2y_test = train_mental_health_models(NSCH)\n",
    "    \n",
    "    # Step 1.3: Evaluate models\n",
    "    model2out2auc = evaluate_models(out2model_type2model, out2X_test, out2y_test)\n",
    "    \n",
    "    # Step 1.4: Calculate estimated probabilities\n",
    "    estimated_prob = calculate_estimated_probabilities(out2model_type2model)\n",
    "    \n",
    "    # Step 2: ACS Data Collection\n",
    "    print(\"\\n===== STEP 2: ACS DATA COLLECTION =====\")\n",
    "    \n",
    "    # Step 2.1: Collect ACS demographic data\n",
    "    acs_data = collect_acs_demographic_data()\n",
    "    age_sex_race_df, employ_insurance_df, language_df, education_df, income_df, family_structure_df = acs_data\n",
    "    \n",
    "    # Step 2.2: Calculate marginal distributions\n",
    "    marginal_distributions = calculate_marginal_distributions(\n",
    "        age_sex_race_df, employ_insurance_df, language_df, \n",
    "        education_df, income_df, family_structure_df\n",
    "    )\n",
    "    \n",
    "    # Step 2.3: Load HYS data\n",
    "    hys_depression_10th = load_hys_data()\n",
    "    \n",
    "    # Step 3: Optimization Model\n",
    "    print(\"\\n===== STEP 3: OPTIMIZATION MODEL =====\")\n",
    "    \n",
    "    # Step 3.1: Run optimization for all counties\n",
    "    depression_10th = run_optimization_for_all_counties(\n",
    "        age_sex_race_df, estimated_prob, marginal_distributions, hys_depression_10th\n",
    "    )\n",
    "    \n",
    "    # Step 4: Map Visualization\n",
    "    print(\"\\n===== STEP 4: MAP VISUALIZATION =====\")\n",
    "    \n",
    "    # Step 4.1: Calculate estimated prevalence\n",
    "    depression_10th = calculate_estimated_prevalence(depression_10th, estimated_prob)\n",
    "    \n",
    "    # Step 4.2: Evaluate model performance\n",
    "    mape, wape, mpe = evaluate_model_performance(depression_10th)\n",
    "    \n",
    "    # Step 4.3: Create bar chart\n",
    "    create_bar_chart(depression_10th)\n",
    "    \n",
    "    # Step 4.4: Create choropleth map\n",
    "    create_choropleth_map(depression_10th)\n",
    "    \n",
    "    print(\"\\nAnalysis complete! Results saved to output files.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eason",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
